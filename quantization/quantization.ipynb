{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "0a9fa558-64a5-4178-b86c-100197f0f41e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torchvision import models, datasets, transforms as T\n",
    "import albumentations as A\n",
    "from albumentations.pytorch import ToTensorV2\n",
    "from statistics import mean\n",
    "import cv2\n",
    "import numpy as np\n",
    "import os\n",
    "import time\n",
    "from matplotlib import pyplot as plt\n",
    "import onnxruntime\n",
    "from onnxruntime.quantization import quantize_static, CalibrationDataReader, QuantType"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "8540202d-4f6d-45ea-a5b2-c1a40f9d2e34",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3, 960, 540)\n"
     ]
    }
   ],
   "source": [
    "x = np.zeros((960, 540, 3))\n",
    "t = x.transpose([2, 0, 1])\n",
    "print(t.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "482c60c8-9a7f-4c36-ac1d-8347f8a489b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_image(image_path, channels=3):\n",
    "    transform = A.Compose(\n",
    "        [\n",
    "            A.Resize(512, 512),\n",
    "            A.Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225))\n",
    "        ]\n",
    "    )\n",
    "    image = cv2.imread(image_path)\n",
    "    image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "    image = transform(image=image)[\"image\"]\n",
    "    image_data = np.asarray(image).astype(np.float32)\n",
    "    image_data = image_data.transpose([2, 0, 1]) # transpose to CHW\n",
    "    \n",
    "    image_data = np.expand_dims(image_data, 0)\n",
    "    return image_data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "9111a0cc-9c13-4d77-93aa-e76d2b3ab1f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_func(images_folder):\n",
    "    image_names = os.listdir(images_folder)\n",
    "    batch_data = []\n",
    "    for image_name in image_names:\n",
    "        image_filepath = os.path.join(images_folder, image_name)\n",
    "        image_data = preprocess_image(image_filepath)\n",
    "        batch_data.append(image_data)\n",
    "    \n",
    "    return batch_data\n",
    "\n",
    "\n",
    "class PackagesDataReader(CalibrationDataReader):\n",
    "    def __init__(self, calibration_image_folder):\n",
    "        self.image_folder = calibration_image_folder\n",
    "        self.preprocess_flag = True\n",
    "        self.enum_data_dicts = []\n",
    "        self.datasize = 0\n",
    "\n",
    "    def get_next(self):\n",
    "        if self.preprocess_flag:\n",
    "            self.preprocess_flag = False\n",
    "            nhwc_data_list = preprocess_func(self.image_folder)\n",
    "            self.datasize = len(nhwc_data_list)\n",
    "            self.enum_data_dicts = iter([{'input': nhwc_data} for nhwc_data in nhwc_data_list])\n",
    "        return next(self.enum_data_dicts, None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "1cd4a4fc-7d99-4b16-a742-e8626e3389bb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ONNX full precision model size (MB): 0.03706550598144531\n",
      "ONNX quantized model size (MB): 0.042102813720703125\n"
     ]
    }
   ],
   "source": [
    "curr_dir = os.getcwd()\n",
    "calibration_data_folder = os.path.join(curr_dir, \"calibration_images\")\n",
    "dr = PackagesDataReader(calibration_data_folder)\n",
    "model_path = os.path.join(curr_dir, \"supersmall400e_noinit.onnx\")\n",
    "quantize_static(\"supersmall400e_noinit.onnx\",\n",
    "                \"supersmall400e_quant.onnx\",\n",
    "                dr, \n",
    "                activation_type=QuantType.QInt8,\n",
    "                weight_type=QuantType.QInt8,)\n",
    "\n",
    "print('ONNX full precision model size (MB):', os.path.getsize(\"supersmall400e_noinit.onnx\")/(1024*1024))\n",
    "print('ONNX quantized model size (MB):', os.path.getsize(\"supersmall400e_quant.onnx\")/(1024*1024))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b768f82e-aa2e-42e7-b1cd-0221df11f879",
   "metadata": {},
   "source": [
    "Now we compare the inference latency of the quantized model with the inference latency of the standard model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "id": "6d6f0ba7-be33-4896-a913-5a5a09db10f2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "latency of standard model:  0.0017830411593119304\n",
      "latency of quantized model:  0.0027505636215209963\n"
     ]
    }
   ],
   "source": [
    "from torch.utils.data import Dataset\n",
    "\n",
    "c_dir = os.getcwd()                 #quantization\n",
    "mt_dir = os.path.dirname(c_dir)     #master_thesis\n",
    "models_path = os.path.join(mt_dir, \"models\")\n",
    "#sys.path.insert(1, models_path)\n",
    "#from fast_scnn import FastSCNN\n",
    "#from super_small_scnn import SuperSmallSCNN\n",
    "#from bisenetv2 import BiSeNetV2\n",
    "\n",
    "\n",
    "class PackagesInferenceDataset(Dataset):\n",
    "    def __init__(self, images_filenames, images_directory, masks_directory, transform=None,):\n",
    "        self.images_filenames = images_filenames\n",
    "        self.images_directory = images_directory\n",
    "        self.masks_directory = masks_directory\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.images_filenames)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        image_filename = self.images_filenames[idx]\n",
    "        image = cv2.imread(os.path.join(self.images_directory, image_filename))\n",
    "        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "        mask = cv2.imread(\n",
    "            os.path.join(self.masks_directory, image_filename), cv2.IMREAD_UNCHANGED,\n",
    "        )\n",
    "        mask = mask.astype(np.float32)\n",
    "        mask[mask == 0.0] = 0.0\n",
    "        mask[mask == 255.0] = 1.0\n",
    "        original_size = tuple(image.shape[:2])\n",
    "        if self.transform is not None:\n",
    "            transformed = self.transform(image=image, mask=mask)\n",
    "            image = transformed[\"image\"]\n",
    "            mask = transformed[\"mask\"]\n",
    "        return image, mask, original_size\n",
    "\n",
    "test_transform = A.Compose(\n",
    "    [\n",
    "        A.Resize(512, 512),\n",
    "        A.Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225)),\n",
    "        ToTensorV2(),\n",
    "    ]\n",
    ")\n",
    "\n",
    "test_images_path = os.path.join(mt_dir, \"full_dataset/test/images\")\n",
    "test_label_path = os.path.join(mt_dir, \"full_dataset/test/labels\")\n",
    "test_images_filenames = [item for item in os.listdir(test_images_path) if item.endswith(\".png\")]\n",
    "test_dataset = PackagesInferenceDataset(images_filenames=test_images_filenames, images_directory=test_images_path, masks_directory=test_label_path, transform=test_transform)\n",
    "\n",
    "test_im = test_dataset[0][0].unsqueeze(0)\n",
    "test_mask = test_dataset[0][1]\n",
    "\n",
    "onnx_quant_path = os.path.join(c_dir, \"supersmall400e_quant.onnx\")\n",
    "onnx_std_path = os.path.join(c_dir, \"supersmall400e_noinit-opt.onnx\")\n",
    "\n",
    "ort_session = onnxruntime.InferenceSession(onnx_quant_path)\n",
    "ort_session_std = onnxruntime.InferenceSession(onnx_std_path)\n",
    "\n",
    "def to_numpy(tensor):\n",
    "    return tensor.detach().cpu().numpy() if tensor.requires_grad else tensor.cpu().numpy()\n",
    "\n",
    "\n",
    "#inference over the entire test dataset one image at a time\n",
    "latency_quant = []\n",
    "outputs_quant = []\n",
    "i = 0\n",
    "for image, mask, (height, width) in test_dataset:\n",
    "    image = image.unsqueeze(0)\n",
    "    ort_inputs = {ort_session.get_inputs()[0].name: to_numpy(image)}\n",
    "    i+=1\n",
    "    start = time.time()\n",
    "    ort_outs = ort_session.run(None, ort_inputs)\n",
    "    lat = time.time()-start\n",
    "    outputs_quant.append([ort_outs, mask])\n",
    "    latency_quant.append(lat)\n",
    "    #print(f\"inference latency on image {i} is {lat}\")\n",
    "    \n",
    "latency_std = []\n",
    "outputs_std = []\n",
    "i = 0\n",
    "for image, mask, (height, width) in test_dataset:\n",
    "    image = image.unsqueeze(0)\n",
    "    ort_inputs = {ort_session_std.get_inputs()[0].name: to_numpy(image)}\n",
    "    i+=1\n",
    "    start = time.time()\n",
    "    ort_outs = ort_session_std.run(None, ort_inputs)\n",
    "    lat = time.time()-start\n",
    "    outputs_std.append([ort_outs, mask])\n",
    "    latency_std.append(lat)\n",
    "    #print(f\"inference latency on image {i} is {lat}\")\n",
    "    \n",
    "print(\"latency of standard model: \", mean(latency_std))\n",
    "print(\"latency of quantized model: \", mean(latency_quant))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "id": "f675e277-841d-468b-9dde-e390ec71717e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def jaccard(input, target):\n",
    "    l_input = input.astype(bool)\n",
    "    l_target = target.astype(bool)\n",
    "    intersection = np.logical_and(l_input, l_target)\n",
    "    union = np.logical_or(l_input, l_target)\n",
    "    iou = np.sum(intersection)/np.sum(union)\n",
    "    return iou"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "id": "2c1207b1-e996-4ac2-88f3-108efee7282e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mean iou of standard model:  0.9726124982048536\n",
      "mean iou of quantized model:  0.7642729827990309\n"
     ]
    }
   ],
   "source": [
    "ious_quant = []\n",
    "ious_std = []\n",
    "tmp = []\n",
    "\n",
    "for pred, g_truth in outputs_quant:\n",
    "    g_truth = g_truth.numpy()\n",
    "    pred = np.array(pred)\n",
    "    pred = pred.squeeze(0) #batch size\n",
    "    pred = pred.squeeze(0) #channels\n",
    "    pred = pred.squeeze(0) # 1 x h x w\n",
    "    pred = (pred >= 0.5) * 1\n",
    "    pred = A.resize(\n",
    "        pred, height=540, width=960, interpolation=cv2.INTER_NEAREST\n",
    "    )\n",
    "    g_truth = A.resize(\n",
    "        g_truth, height=540, width=960, interpolation=cv2.INTER_NEAREST\n",
    "    )\n",
    "    tmp.append(pred-g_truth)\n",
    "    ious_quant.append(jaccard(pred, g_truth))\n",
    "    \n",
    "\n",
    "\n",
    "for pred, g_truth in outputs_std:\n",
    "    g_truth = g_truth.numpy()\n",
    "    outs = np.array(pred)\n",
    "    outs = outs.squeeze(0) #batch size\n",
    "    outs = outs.squeeze(0) #channels\n",
    "    outs = outs.squeeze(0) # 1 x h x w\n",
    "    outs = (outs >= 0.5) * 1\n",
    "    pred = A.resize(\n",
    "        outs, height=540, width=960, interpolation=cv2.INTER_NEAREST\n",
    "    )\n",
    "    g_truth = A.resize(\n",
    "        g_truth, height=540, width=960, interpolation=cv2.INTER_NEAREST\n",
    "    )\n",
    "    ious_std.append(jaccard(pred, g_truth))\n",
    "\n",
    "    \n",
    "print(\"mean iou of standard model: \", mean(ious_std))\n",
    "print(\"mean iou of quantized model: \", mean(ious_quant))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {},
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
